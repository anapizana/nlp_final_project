{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import math \n",
    "import re \n",
    "import pickle\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from sklearn import preprocessing\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   0.   0.   0.7  0.7  0.7  0.7  0.7  0.7  0.7  0.7  0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.7  0.7\n",
      "  0.7  0.7  0.7  0.7  0.7]\n",
      "[ 0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Converting training to no numbers and no punctuation\n"
     ]
    }
   ],
   "source": [
    "# load the dataset \n",
    "train_dataset = pd.read_csv(\"train_dataset.csv\")\n",
    "test_dataset = pd.read_csv(\"test_dataset.csv\")\n",
    "\n",
    "# extract the notes and the mortality labels \n",
    "train_notes = train_dataset['note'].as_matrix()\n",
    "test_notes = test_dataset['note'].as_matrix()\n",
    "train_labels = train_dataset['label'].as_matrix().astype(float)\n",
    "test_labels = test_dataset['label'].as_matrix().astype(float)\n",
    "\n",
    "# extract the sofa scores \n",
    "sofa_train_labels = train_dataset['sofa'].as_matrix()\n",
    "sofa_test_labels = test_dataset['sofa'].as_matrix()\n",
    "\n",
    "# combine into one set of notes \n",
    "all_notes = np.concatenate((train_notes, test_notes), axis=0)\n",
    "all_labels = np.concatenate((train_labels, test_labels), axis=0)\n",
    "\n",
    "# combine all the sofa notes \n",
    "sofa_labels = np.concatenate((sofa_train_labels, sofa_test_labels), axis=0)\n",
    "\n",
    "# label smoothing\n",
    "smooth_labels = np.copy(all_labels)\n",
    "smooth_labels[smooth_labels > 0.0] = 0.7\n",
    "print(smooth_labels[:50])\n",
    "print(all_labels[:50])\n",
    "\n",
    "# convert into clean format\n",
    "print(\"Converting training to no numbers and no punctuation\")\n",
    "for i in range(len(all_notes)):\n",
    "    note = all_notes[i].lower()\n",
    "    string = re.sub(\"\\d+\", \"\", note)\n",
    "    words = \" \".join(re.findall(r'\\w+', string))\n",
    "    all_notes[i] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106812\n",
      "106812\n",
      "106812\n",
      "[  0.   0.   0.   3.   2.   0.   0.   2.   2.   2.   2.   0.  14.  13.  13.\n",
      "   9.   7.   7.   2.   2.]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "\n",
    "# extract the actions\n",
    "train_actions = train_dataset['action'].as_matrix()\n",
    "test_actions = test_dataset['action'].as_matrix()\n",
    "all_actions = np.concatenate((train_actions, test_actions), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notes length 106812\n",
      "max length of sequence 5047\n"
     ]
    }
   ],
   "source": [
    "# build the tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(all_notes)\n",
    "vocab_size = len(t.word_index) + 1 \n",
    "\n",
    "# integer encoding\n",
    "encoded_notes = t.texts_to_sequences(all_notes)\n",
    "\n",
    "print(\"notes length\", len(encoded_notes))\n",
    "\n",
    "max_len = 0 \n",
    "for string in encoded_notes:\n",
    "    length = len(string)\n",
    "    if length > max_len:\n",
    "        max_len = length\n",
    "print(\"max length of sequence\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 45725\n",
      "[[12 24 28 ...,  0  0  0]\n",
      " [12 24 28 ...,  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(\"vocab size\", vocab_size)\n",
    "max_length = 3000\n",
    "padded_notes = pad_sequences(encoded_notes, maxlen=max_length, padding='post')\n",
    "print(padded_notes[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12986 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load embeddings\n",
    "f = gzip.open('word_vectors.txt.gz', 'r')\n",
    "wv_text = [ ]\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    wv_text.append(line.strip())\n",
    "\n",
    "word_to_vec = {}\n",
    "\n",
    "for line in wv_text:\n",
    "    parts = line.split()\n",
    "    word = parts[0]\n",
    "    vector = np.array([float(v) for v in parts[1:]])\n",
    "    word_to_vec[word] = vector\n",
    "\n",
    "print('Loaded %s word vectors.' % len(word_to_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create embedding matrix \n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = word_to_vec.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06728151  0.0133113  -0.0587122   0.05504927  0.01670193 -0.04523229\n",
      " -0.02211876  0.03265653 -0.00706662  0.52338961]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106812\n",
      "106812\n",
      "(10682, 3000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for splitting up samples \n",
    "print(len(all_actions))\n",
    "print(len(padded_notes))\n",
    "all_features = np.concatenate((np.array([all_actions]).T, padded_notes), axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train_actions = X_train[:, 0]\n",
    "X_train_embeddings = X_train[:, 1:]\n",
    "X_test_actions = X_test[:, 0]\n",
    "X_test_embeddings = X_test[:, 1:]\n",
    "\n",
    "print(X_test_embeddings.shape)\n",
    "\n",
    "# kf = KFold(n_splits=5, random_state=None, shuffle=False) \n",
    "\n",
    "# for k, (train, test) in enumerate(k_fold.split(padded_notes)):\n",
    "#     lasso_cv.fit(padded_notes[train], all_labels[train])\n",
    "#     print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n",
    "#           format(k, lasso_cv.alpha_, lasso_cv.score(padded_notes[test], all_labels[test])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### CODE FOR MODEL WITHOUT ACTION \n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "model.add(e)\n",
    "\n",
    "# higher dropout\n",
    "model.add(LSTM(128, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train_embeddings, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(X_test_embeddings, y_test))\n",
    "score, acc = model.evaluate(X_test_embeddings, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### CODE FOR MODEL WITH ACTION \n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Flatten, Bidirectional\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "print(len(X_train_embeddings), 'train sequences')\n",
    "print(len(X_test_embeddings), 'test sequences')\n",
    "\n",
    "print('Build model...')\n",
    "text_model = Sequential()\n",
    "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "text_model.add(e)\n",
    "text_model.add(LSTM(128, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "action_model = Sequential()\n",
    "action_model.add(Dense(1, input_shape=(1,), activation='sigmoid'))\n",
    "\n",
    "model = Sequential()\n",
    "merged = Merge([text_model, action_model], mode = 'concat')\n",
    "model.add(merged)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit([X_train_embeddings, X_train_actions], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=([X_test_embeddings, X_test_actions.T], y_test))\n",
    "score, acc = model.evaluate([X_test_embeddings, X_test_actions.T], y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
