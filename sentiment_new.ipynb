{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model inputs: [label: mortality, action, rl_features, note_string]\n",
    "Model output: mortality \n",
    "\n",
    "1. Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "import math \n",
    "import re \n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Bag of words model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train notes 80185\n",
      "# of test notes 26627\n",
      "Created count vectorizer\n",
      "Converting to no numbers\n",
      "nursing other report npn p a this is a yo female with no pmh who presented to ew with rigors temp of pt has seen her pcp and was given azithromycin for pneumonia cxr in ew showed lll pneumonia given mg levaquin gm tylenol and l ivf s as she was hypotensive to high s systolic transferred here to msicu for closer monitoring before going to floor at this time all vital signs are stable a ox non productive cough noted crackles in lll otherwise clear pt satting on r a abd soft bs g piv intact awaiting md assessment sepsis vs pneumonia w dehydration nursing other report patient has been hemodynamically stable see careview sr st without ectopy denies any chest pain no sob lung sounds clear crackles at lll occasional productive cough whitish yellow in small amount no sob sats at room air voiding adequate amount of clear yellow urine on regular diet with fair appetite continues on levofloxacin mgs po qhrs x days day afebrile tmax denies any headache oob to commode gait steady denies any dizziness patient anxious of going home i feel better im not even suppose to be here in the icu i have travel planned for tuesday morning patient is flying to state for days vacation plan at morning rounds is to discharge patient home after pm cbc results are reviewed team made aware of results wbc wnl discharge instructions given dr last name stitle last name namepattern discussed discharge plan with patient accompanied by husband to home refused wheelchair transport ambulatory in good condition\n"
     ]
    }
   ],
   "source": [
    "### Bag of Words model for sentiment analysis\n",
    "\n",
    "# => compile all the words that appear in the training set of notes into a dictionary, to produce a list of 'd' unique words\n",
    "# => transform each of the reviews into a feature vector \n",
    "# by setting the ith coordinate to 1 if the ith word in the dictionary appears in the note\n",
    "# => print # of unique words observed \n",
    "# => remove stop words \n",
    "# => print top 10 words\n",
    "# TEST two version: (1) binary, (2) based on word counts, (3) unigram vs. bigrams \n",
    "\n",
    "# => learn a LogisticRegression classifier and evaluate \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files \n",
    "from sklearn import linear_model \n",
    "import matplotlib.pyplot as plt \n",
    "import random \n",
    "\n",
    "# load the dataset \n",
    "train_dataset = pd.read_csv(\"train_dataset.csv\")\n",
    "test_dataset = pd.read_csv(\"test_dataset.csv\")\n",
    "train_notes = train_dataset['note'].as_matrix()\n",
    "test_notes = test_dataset['note'].as_matrix()\n",
    "print \"# of train notes\", len(train_notes)\n",
    "print \"# of test notes\", len(test_notes)\n",
    "\n",
    "# converting train and test to clean format \n",
    "print \"Converting training to no numbers\"\n",
    "for i in range(len(train_notes)):\n",
    "    note = train_notes[i].lower()\n",
    "    string = re.sub(\"\\d+\", \"\", note)\n",
    "    train_notes[i] = \" \".join(re.findall(r'\\w+', string))\n",
    "\n",
    "print \"Converting testing to no numbers\"\n",
    "for i in range(len(test_notes)):\n",
    "    note = test_notes[i].lower()\n",
    "    string = re.sub(\"\\d+\", \"\", note)\n",
    "    test_notes[i] = \" \".join(re.findall(r'\\w+', string))\n",
    "    \n",
    "print \"Finished converting.\"\n",
    "print train_notes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary:  845521\n",
      "[0.74732414466518948, 0.7247155143275622, 0.71810568220227589, 0.71761745596574911, 0.71735456491531158]\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary\n",
    "vocab = defaultdict()\n",
    "for i, name in enumerate(cv.get_feature_names()):\n",
    "    vocab[name] = 1\n",
    "print \"Size of Vocabulary: \", len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross-validation (5 parts) --> sklearn (score: training) ===> unigram, bigram, unigram/bigram  \n",
    "\n",
    "# Test validation score \n",
    "\n",
    "# Top features (noisy) => confidence intervals, if sig. (choose significant ones)\n",
    "\n",
    "# unigram, bigram, trigram\n",
    "\n",
    "# Random Forest \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features\n",
      "dnr\n",
      "intubated current\n",
      "meeting\n",
      "pt tx\n",
      "pronounced\n",
      "mets\n",
      "duoderm\n",
      "report correction\n",
      "family bedside\n",
      "psv cpap\n",
      "cmo\n",
      "receiving cc\n",
      "expired\n",
      "fi\n",
      "noted nursing\n",
      "admission note\n",
      "current abg\n",
      "wean fio\n",
      "morphine gtt\n",
      "pcv\n",
      "discussion\n",
      "psv weaned\n",
      "oliguria\n",
      "remained psv\n",
      "secretions abgs\n",
      "wife son\n",
      "cvvhd\n",
      "support changes\n",
      "tx alb\n",
      "met acidosis\n"
     ]
    }
   ],
   "source": [
    "num_top = 30\n",
    "coefs=best_lr.coef_[0]\n",
    "top = np.argpartition(coefs, -num_top)[-num_top:]\n",
    "top_sorted=top[np.argsort(coefs[top])]\n",
    "names = feature_names[top_sorted]\n",
    "\n",
    "print \"Top Features\"\n",
    "for i in range(num_top):\n",
    "    index = num_top - 1 - i\n",
    "    print names[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Scores\n",
      "Scores for Unigram [0.73958763660945659, 0.70327111578472978, 0.69636083674465765, 0.6972246216246667, 0.69313103241071095]\n",
      "Scores for Bigram [0.7548728733991813, 0.72648063995192846, 0.71686633867878469, 0.71367409020918615, 0.71510121305441843]\n",
      "Scores for Unigram and Bigram [0.74732414466518948, 0.7247155143275622, 0.71810568220227589, 0.71761745596574911, 0.71735456491531158]\n",
      "Training Scores\n",
      "Scores for Unigram [0.93944004489617761, 0.98457317453389037, 0.99035979297873666, 0.99068404315021508, 0.99412608343206332]\n",
      "Scores for Bigram [0.99462492984972251, 0.99972563447028751, 0.99975057679117041, 0.99975057679117041, 0.99975057679117041]\n",
      "Scores for Unigram and Bigram [0.99600922865872665, 0.99972563447028751, 0.99975057679117041, 0.99975057679117041, 0.99975057679117041]\n"
     ]
    }
   ],
   "source": [
    "# Test Unigrams, Bigrams, and Unigram + Bigrams \n",
    "\n",
    "# extract the labels \n",
    "train_labels = train_dataset['label'].as_matrix()\n",
    "test_labels = test_dataset['label'].as_matrix()\n",
    "\n",
    "# initialize CVs\n",
    "cv_unigram = CountVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n",
    "cv_bigram = CountVectorizer(ngram_range=(2, 2), stop_words=\"english\")\n",
    "cv = CountVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
    "\n",
    "# fit to training data\n",
    "cv_unigram_fit = cv_unigram.fit_transform(train_notes)\n",
    "cv_bigram_fit = cv_bigram.fit_transform(train_notes)\n",
    "cv_fit = cv.fit_transform(train_notes)\n",
    "\n",
    "# testing data \n",
    "cv_unigram_test = cv_unigram.transform(test_notes)\n",
    "cv_bigram_test = cv_bigram.transform(test_notes)\n",
    "cv_test = cv.transform(test_notes)\n",
    "\n",
    "# create the LR models \n",
    "constants = [1e-1, 1e1, 1e3, 1e5, 1e7] \n",
    "\n",
    "# testing scores \n",
    "scores_unigram = []\n",
    "scores_bigram = []\n",
    "scores_unibigram = []\n",
    "\n",
    "# models \n",
    "models_unigram = []\n",
    "models_bigram = []\n",
    "models_unibigram = []\n",
    "\n",
    "# training scores \n",
    "train_scores_u = []\n",
    "train_scores_b = []\n",
    "train_scores_ub = []\n",
    "\n",
    "for c in constants:\n",
    "    # unigram\n",
    "    lr_unigram = linear_model.LogisticRegression(C=c)\n",
    "    lr_unigram.fit(cv_unigram_fit, train_labels)\n",
    "    train_scores_u.append(lr_unigram.score(cv_unigram_fit, train_labels))\n",
    "    scores_unigram.append(lr_unigram.score(cv_unigram_test, test_labels))\n",
    "    models_unigram.append(lr_unigram)\n",
    "    \n",
    "    # bigram\n",
    "    lr_b = linear_model.LogisticRegression(C=c)\n",
    "    lr_b.fit(cv_bigram_fit, train_labels)\n",
    "    train_scores_b.append(lr_b.score(cv_bigram_fit, train_labels))\n",
    "    scores_bigram.append(lr_b.score(cv_bigram_test, test_labels))\n",
    "    models_bigram.append(lr_b)\n",
    "    \n",
    "    # unibigram\n",
    "    lr_ub = linear_model.LogisticRegression(C=c)\n",
    "    lr_ub.fit(cv_fit, train_labels)\n",
    "    train_scores_ub.append(lr_ub.score(cv_fit, train_labels))\n",
    "    scores_unibigram.append(lr_ub.score(cv_test, test_labels))\n",
    "    models_unibigram.append(lr_ub)\n",
    "\n",
    "print \"Testing Scores\"\n",
    "print \"Scores for Unigram\", scores_unigram\n",
    "print \"Scores for Bigram\", scores_bigram \n",
    "print \"Scores for Unigram and Bigram\", scores_unibigram\n",
    "\n",
    "print \"Training Scores\"\n",
    "print \"Scores for Unigram\", train_scores_u\n",
    "print \"Scores for Bigram\", train_scores_b\n",
    "print \"Scores for Unigram and Bigram\", train_scores_ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features for Unigram\n",
      "Top Features\n",
      "oliguria\n",
      "pronounced\n",
      "nonreactive\n",
      "mets\n",
      "pitressin\n",
      "flows\n",
      "bagged\n",
      "excoriation\n",
      "space\n",
      "metastatic\n",
      "nectar\n",
      "resolve\n",
      "cyanotic\n",
      "tobramycin\n",
      "olanzapine\n",
      "induce\n",
      "beneprotein\n",
      "fenoldopam\n",
      "commenced\n",
      "adaptic\n",
      "steroid\n",
      "wanting\n",
      "dnr\n",
      "generaly\n",
      "flowby\n",
      "attack\n",
      "defib\n",
      "meeting\n",
      "intermit\n",
      "duoderm\n",
      "Top Features for Bigram\n",
      "Top Features\n",
      "family meeting\n",
      "family bedside\n",
      "dnr dni\n",
      "morphine gtt\n",
      "intubated current\n",
      "met acidosis\n",
      "report correction\n",
      "pt tx\n",
      "pt dnr\n",
      "palliative care\n",
      "receiving cc\n",
      "comfort measures\n",
      "maintained range\n",
      "noted nursing\n",
      "wife son\n",
      "skin duoderm\n",
      "secretions abgs\n",
      "psv cpap\n",
      "current abg\n",
      "pt expired\n",
      "wean fio\n",
      "rr peep\n",
      "support changes\n",
      "pt pronounced\n",
      "remained psv\n",
      "family present\n",
      "essentially unchanged\n",
      "lung ca\n",
      "code status\n",
      "cpap fio\n",
      "Top Features for Unigram and Bigram\n",
      "Top Features\n",
      "aggitaed\n",
      "airway seizures\n",
      "afternoon oozing\n",
      "afebrile rehab\n",
      "agreeing turned\n",
      "added response\n",
      "able diet\n",
      "actually item\n",
      "ammonia action\n",
      "afebrile overnoc\n",
      "afternoon appreciated\n",
      "allergies fever\n",
      "ache chest\n",
      "andwered appropriatly\n",
      "aggatroban\n",
      "admission anemic\n",
      "able recount\n",
      "add\n",
      "access angiocaths\n",
      "abd applied\n",
      "amoxicillin suspected\n",
      "antibiotics cuture\n",
      "action doing\n",
      "additional manual\n",
      "added rate\n",
      "abgs suction\n",
      "acid team\n",
      "afebrile glucose\n",
      "admission runs\n",
      "action oriented\n"
     ]
    }
   ],
   "source": [
    "### LR ANALYSIS \n",
    "\n",
    "best_unigram_model = models_unigram[0]\n",
    "best_bigram_model = models_bigram[0]\n",
    "best_unibigram_model = models_unigram[0]\n",
    "\n",
    "feature_names_unigram = np.array(cv_unigram.get_feature_names())\n",
    "feature_names_bigram = np.array(cv_bigram.get_feature_names())\n",
    "feature_names_unibigram = np.array(cv.get_feature_names())\n",
    "\n",
    "num_top = 30\n",
    "def find_top_features(model, feature_names):\n",
    "    coefs=model.coef_[0]\n",
    "    top = np.argpartition(coefs, -num_top)[-num_top:]\n",
    "    top_sorted=top[np.argsort(coefs[top])]\n",
    "    names = feature_names[top_sorted]\n",
    "\n",
    "    print \"Top Features\"\n",
    "    for i in range(num_top):\n",
    "        index = num_top - 1 - i\n",
    "        print names[index]\n",
    "\n",
    "print \"Top Features for Unigram\"\n",
    "unigram_features = np.array(cv_unigram.get_feature_names())\n",
    "find_top_features(best_unigram_model, unigram_features)\n",
    "\n",
    "print \"Top Features for Bigram\"\n",
    "bigram_features = np.array(cv_bigram.get_feature_names())\n",
    "find_top_features(best_bigram_model, bigram_features)\n",
    "\n",
    "print \"Top Features for Unigram and Bigram\"\n",
    "unibigram_features = np.array(cv.get_feature_names())\n",
    "find_top_features(best_unibigram_model, unibigram_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ### SVM Classifier \n",
    "# from sklearn import svm\n",
    "\n",
    "# # testing scores \n",
    "# test_svm_u = []\n",
    "# test_svm_b = []\n",
    "# test_svm_ub = []\n",
    "\n",
    "# # models \n",
    "# models_svm_u = []\n",
    "# models_svm_b = []\n",
    "# models_svm_ub = [] \n",
    "\n",
    "# # training scores \n",
    "# train_svm_u = []\n",
    "# train_svm_b = []\n",
    "# train_svm_ub = [] \n",
    "\n",
    "# for c in constants:\n",
    "#     # unigram\n",
    "#     svm_u = svm.SVC()\n",
    "#     svm_u.fit(cv_unigram_fit, train_labels)\n",
    "#     train_svm_u.append(svm_u.score(cv_unigram_fit, train_labels))\n",
    "#     test_svm_u.append(svm_u.score(cv_unigram_test, test_labels))\n",
    "#     models_svm_u.append(svm_u)\n",
    "    \n",
    "#     # bigram\n",
    "#     svm_b = svm.SVC()\n",
    "#     svm_b.fit(cv_bigram_fit, train_labels)\n",
    "#     train_svm_b.append(svm_b.score(cv_bigram_fit, train_labels))\n",
    "#     test_svm_b.append(svm_b.score(cv_bigram_test, test_labels))\n",
    "#     models_svm_b.append(svm_b)\n",
    "    \n",
    "#     # unibigram\n",
    "#     svm_ub = svm.SVC()\n",
    "#     svm_ub.fit(cv_fit, train_labels)\n",
    "#     train_svm_ub.append(svm_ub.score(cv_fit, train_labels))\n",
    "#     test_svm_ub.append(svm_ub.score(cv_test, test_labels))\n",
    "#     models_svm_ub.append(svm_ub)\n",
    "\n",
    "# print \"Testing Scores\"\n",
    "# print \"Scores for Unigram\", test_svm_u\n",
    "# print \"Scores for Bigram\", test_svm_b \n",
    "# print \"Scores for Unigram and Bigram\", test_svm_ub\n",
    "\n",
    "# print \"Training Scores\"\n",
    "# print \"Scores for Unigram\", train_svm_u\n",
    "# print \"Scores for Bigram\", train_svm_b\n",
    "# print \"Scores for Unigram and Bigram\", train_svm_ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### SVM ANALYSIS \n",
    "\n",
    "# best_unigram_svm = models_svm_u[]\n",
    "# best_bigram_svm = models_svm_b[]\n",
    "# best_unibigram_svm = models_svm_ub[]\n",
    "\n",
    "# print \"Top Features for SVM Unigram\"\n",
    "# find_top_features(best_unigram_svm, unigram_features)\n",
    "\n",
    "# print \"Top Features for SVM Bigram\"\n",
    "# find_top_features(best_bigram_svm, bigram_features)\n",
    "\n",
    "# print \"Top Features for SVM Unigram and Bigram\"\n",
    "# find_top_features(best_unibigram_svm, unibigram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Scores for Tree Classifiers\n",
      "Scores for DT []\n",
      "Scores for ET [0.69974086453599726, 0.74649791564952872, 0.75051639313478802, 0.69974086453599726, 0.74649791564952872, 0.75051639313478802, 0.69974086453599726, 0.74649791564952872, 0.75051639313478802, 0.69974086453599726, 0.74649791564952872, 0.75051639313478802, 0.69974086453599726, 0.74649791564952872, 0.75051639313478802]\n",
      "Scores for RF []\n",
      "Training Scores\n",
      "Scores for DT []\n",
      "Scores for ET [0.99975057679117041, 0.99975057679117041, 0.99927667269439424, 0.99975057679117041, 0.99975057679117041, 0.99927667269439424, 0.99975057679117041, 0.99975057679117041, 0.99927667269439424, 0.99975057679117041, 0.99975057679117041, 0.99927667269439424, 0.99975057679117041, 0.99975057679117041, 0.99927667269439424]\n",
      "Scores for RF []\n"
     ]
    }
   ],
   "source": [
    "### RANDOM FORESTS \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# testing scores \n",
    "test_t_u = []\n",
    "test_t_b = []\n",
    "test_t_ub = []\n",
    "\n",
    "# models \n",
    "models_t_u = []\n",
    "models_t_b = []\n",
    "models_t_ub = [] \n",
    "\n",
    "# training scores \n",
    "train_t_u = []\n",
    "train_t_b = []\n",
    "train_t_ub = [] \n",
    "\n",
    "for c in constants:\n",
    "    \n",
    "    # DT\n",
    "    dt = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "    dt.fit(cv_bigram_fit, train_labels)\n",
    "    train_t_b.append(dt.score(cv_bigram_fit, train_labels))\n",
    "    test_t_b.append(dt.score(cv_bigram_test, test_labels))\n",
    "    models_t_b.append(dt)\n",
    "    \n",
    "    # ET\n",
    "    et = ExtraTreesClassifier(max_depth=None, min_samples_split=2, random_state=0) \n",
    "    et.fit(cv_bigram_fit, train_labels)\n",
    "    train_t_b.append(et.score(cv_bigram_fit, train_labels))\n",
    "    test_t_b.append(et.score(cv_bigram_test, test_labels))\n",
    "    models_t_b.append(et)\n",
    "    \n",
    "    # RF\n",
    "    rf = RandomForestClassifier(max_depth=None, min_samples_split=2, random_state=0) \n",
    "    rf.fit(cv_bigram_fit, train_labels)\n",
    "    train_t_b.append(rf.score(cv_bigram_fit, train_labels))\n",
    "    test_t_b.append(rf.score(cv_bigram_test, test_labels))\n",
    "    models_t_b.append(rf)\n",
    "\n",
    "print \"Testing Scores for Tree Classifiers\"\n",
    "print \"Scores for DT\", test_t_u\n",
    "print \"Scores for ET\", test_t_b \n",
    "print \"Scores for RF\", test_t_ub\n",
    "\n",
    "print \"Training Scores\"\n",
    "print \"Scores for DT\", train_t_u\n",
    "print \"Scores for ET\", train_t_b\n",
    "print \"Scores for RF\", train_t_ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Random Forests ANALYSIS \n",
    "best_unigram_rf = models_t_b[0]\n",
    "best_bigram_rf = models_t_b[1]\n",
    "best_unibigram_rf = models_t_b[2]\n",
    "\n",
    "print \"Top Features for RF Unigram\"\n",
    "find_top_features(best_unigram_rf, unigram_features)\n",
    "\n",
    "print \"Top Features for RF Bigram\"\n",
    "find_top_features(best_bigram_rf, bigram_features)\n",
    "\n",
    "print \"Top Features for RF Unigram and Bigram\"\n",
    "find_top_features(best_unibigram_rf, unibigram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
