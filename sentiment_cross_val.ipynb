{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model inputs: [label: mortality, action, rl_features, note_string]\n",
    "Model output: mortality \n",
    "\n",
    "1. Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "import math \n",
    "import re \n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Bag of words model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   0.   0.   0.7  0.7  0.7  0.7  0.7  0.7  0.7  0.7  0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.7  0.7\n",
      "  0.7  0.7  0.7  0.7  0.7]\n",
      "[ 0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Converting training to no numbers and no punctuation\n"
     ]
    }
   ],
   "source": [
    "### Bag of Words model for sentiment analysis\n",
    "\n",
    "# => compile all the words that appear in the training set of notes into a dictionary, to produce a list of 'd' unique words\n",
    "# => transform each of the reviews into a feature vector \n",
    "# by setting the ith coordinate to 1 if the ith word in the dictionary appears in the note\n",
    "# => print # of unique words observed \n",
    "# => remove stop words \n",
    "# => print top 10 words\n",
    "# TEST two version: (1) binary, (2) based on word counts, (3) unigram vs. bigrams \n",
    "\n",
    "# => learn a LogisticRegression classifier and evaluate \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files \n",
    "from sklearn import linear_model \n",
    "import matplotlib.pyplot as plt \n",
    "import random \n",
    "\n",
    "# load the dataset \n",
    "train_dataset = pd.read_csv(\"train_dataset.csv\")\n",
    "test_dataset = pd.read_csv(\"test_dataset.csv\")\n",
    "\n",
    "# extract the notes and the mortality labels \n",
    "train_notes = train_dataset['note'].as_matrix()\n",
    "test_notes = test_dataset['note'].as_matrix()\n",
    "train_labels = train_dataset['label'].as_matrix().astype(float)\n",
    "test_labels = test_dataset['label'].as_matrix().astype(float)\n",
    "\n",
    "# extract the sofa scores \n",
    "sofa_train_labels = train_dataset['sofa'].as_matrix()\n",
    "sofa_test_labels = test_dataset['sofa'].as_matrix()\n",
    "\n",
    "# combine into one set of notes \n",
    "all_notes = np.concatenate((train_notes, test_notes), axis=0)\n",
    "all_labels = np.concatenate((train_labels, test_labels), axis=0)\n",
    "\n",
    "# combine all the sofa notes \n",
    "sofa_labels = np.concatenate((sofa_train_labels, sofa_test_labels), axis=0)\n",
    "\n",
    "# label smoothing\n",
    "smooth_labels = np.copy(all_labels)\n",
    "smooth_labels[smooth_labels > 0.0] = 0.7\n",
    "print(smooth_labels[:50])\n",
    "print(all_labels[:50])\n",
    "\n",
    "# convert into clean format\n",
    "print(\"Converting training to no numbers and no punctuation\")\n",
    "for i in range(len(all_notes)):\n",
    "    note = all_notes[i].lower()\n",
    "    string = re.sub(\"\\d+\", \"\", note)\n",
    "    words = \" \".join(re.findall(r'\\w+', string))\n",
    "    all_notes[i] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_actions = train_dataset['action'].as_matrix()\n",
    "test_actions = test_dataset['action'].as_matrix()\n",
    "all_actions = np.concatenate((train_actions, test_actions), axis=0)\n",
    "all_features = np.concatenate((np.array([all_actions]).T, padded_notes), axis=1)\n",
    "print len(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary:  845521\n",
      "[0.74732414466518948, 0.7247155143275622, 0.71810568220227589, 0.71761745596574911, 0.71735456491531158]\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary\n",
    "vocab = defaultdict()\n",
    "for i, name in enumerate(cv.get_feature_names()):\n",
    "    vocab[name] = 1\n",
    "print \"Size of Vocabulary: \", len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106812\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation (5 parts) --> sklearn (score: training) ===> unigram, bigram, unigram/bigram  \n",
    "\n",
    "# Test validation score \n",
    "\n",
    "# Top features (noisy) => confidence intervals, if sig. (choose significant ones)\n",
    "\n",
    "# unigram, bigram, trigram\n",
    "\n",
    "# Random Forest \n",
    "print len(all_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features\n",
      "dnr\n",
      "intubated current\n",
      "meeting\n",
      "pt tx\n",
      "pronounced\n",
      "mets\n",
      "duoderm\n",
      "report correction\n",
      "family bedside\n",
      "psv cpap\n",
      "cmo\n",
      "receiving cc\n",
      "expired\n",
      "fi\n",
      "noted nursing\n",
      "admission note\n",
      "current abg\n",
      "wean fio\n",
      "morphine gtt\n",
      "pcv\n",
      "discussion\n",
      "psv weaned\n",
      "oliguria\n",
      "remained psv\n",
      "secretions abgs\n",
      "wife son\n",
      "cvvhd\n",
      "support changes\n",
      "tx alb\n",
      "met acidosis\n"
     ]
    }
   ],
   "source": [
    "num_top = 30\n",
    "coefs=best_lr.coef_[0]\n",
    "top = np.argpartition(coefs, -num_top)[-num_top:]\n",
    "top_sorted=top[np.argsort(coefs[top])]\n",
    "names = feature_names[top_sorted]\n",
    "\n",
    "print \"Top Features\"\n",
    "for i in range(num_top):\n",
    "    index = num_top - 1 - i\n",
    "    print names[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal 0.754622478116\n",
      "Smooth 0.119739736928\n",
      "Normal 0.752796891822\n",
      "Smooth 0.123905818471\n",
      "Normal 0.756670723715\n",
      "Smooth 0.118387791405\n",
      "Normal"
     ]
    }
   ],
   "source": [
    "# Test Unigrams, Bigrams, and Unigram + Bigrams \n",
    "\n",
    "# cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# initialize CVs\n",
    "cv_bigram = CountVectorizer(ngram_range=(2, 2), stop_words=\"english\")\n",
    "\n",
    "# kf\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "scores1 = []\n",
    "scores2 = []\n",
    "scores3 = []\n",
    "models1 = []\n",
    "models2 = []\n",
    "models3 = []\n",
    "train1 = []\n",
    "train2 = []\n",
    "train3 = []\n",
    "# try smooth_labels and all_labels\n",
    "for k, (train, test) in enumerate(kf.split(all_notes, all_labels)):\n",
    "    \n",
    "    # normal labels \n",
    "    cv_bigram_fit = cv_bigram.fit_transform(all_notes[train])\n",
    "    cv_bigram_test = cv_bigram.transform(all_notes[test])\n",
    "    \n",
    "    lr_b = linear_model.LogisticRegression(C=1e-1)\n",
    "    lr_b.fit(cv_bigram_fit, all_labels[train])\n",
    "    train1.append(lr_b.score(cv_bigram_fit, all_labels[train]))\n",
    "    score = lr_b.score(cv_bigram_test, all_labels[test])\n",
    "    print \"Normal\", score \n",
    "    scores1.append(score)\n",
    "    models1.append(lr_b)\n",
    "    \n",
    "    # SOFA labels\n",
    "    cv_bigram_fit = cv_bigram.fit_transform(all_notes[train])\n",
    "    cv_bigram_test = cv_bigram.transform(all_notes[test])\n",
    "    \n",
    "    lr_b = linear_model.LogisticRegression(C=1e-1)\n",
    "    lr_b.fit(cv_bigram_fit, sofa_labels[train])\n",
    "    train3.append(lr_b.score(cv_bigram_fit, sofa_labels[train]))\n",
    "    score = lr_b.score(cv_bigram_test, sofa_labels[test])\n",
    "    print \"Smooth\", score \n",
    "    scores3.append(score)\n",
    "    models3.append(lr_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features for Unigram\n",
      "Top Features\n",
      "oliguria\n",
      "pronounced\n",
      "nonreactive\n",
      "mets\n",
      "pitressin\n",
      "flows\n",
      "bagged\n",
      "excoriation\n",
      "space\n",
      "metastatic\n",
      "nectar\n",
      "resolve\n",
      "cyanotic\n",
      "tobramycin\n",
      "olanzapine\n",
      "induce\n",
      "beneprotein\n",
      "fenoldopam\n",
      "commenced\n",
      "adaptic\n",
      "steroid\n",
      "wanting\n",
      "dnr\n",
      "generaly\n",
      "flowby\n",
      "attack\n",
      "defib\n",
      "meeting\n",
      "intermit\n",
      "duoderm\n",
      "Top Features for Bigram\n",
      "Top Features\n",
      "family meeting\n",
      "family bedside\n",
      "dnr dni\n",
      "morphine gtt\n",
      "intubated current\n",
      "met acidosis\n",
      "report correction\n",
      "pt tx\n",
      "pt dnr\n",
      "palliative care\n",
      "receiving cc\n",
      "comfort measures\n",
      "maintained range\n",
      "noted nursing\n",
      "wife son\n",
      "skin duoderm\n",
      "secretions abgs\n",
      "psv cpap\n",
      "current abg\n",
      "pt expired\n",
      "wean fio\n",
      "rr peep\n",
      "support changes\n",
      "pt pronounced\n",
      "remained psv\n",
      "family present\n",
      "essentially unchanged\n",
      "lung ca\n",
      "code status\n",
      "cpap fio\n",
      "Top Features for Unigram and Bigram\n",
      "Top Features\n",
      "aggitaed\n",
      "airway seizures\n",
      "afternoon oozing\n",
      "afebrile rehab\n",
      "agreeing turned\n",
      "added response\n",
      "able diet\n",
      "actually item\n",
      "ammonia action\n",
      "afebrile overnoc\n",
      "afternoon appreciated\n",
      "allergies fever\n",
      "ache chest\n",
      "andwered appropriatly\n",
      "aggatroban\n",
      "admission anemic\n",
      "able recount\n",
      "add\n",
      "access angiocaths\n",
      "abd applied\n",
      "amoxicillin suspected\n",
      "antibiotics cuture\n",
      "action doing\n",
      "additional manual\n",
      "added rate\n",
      "abgs suction\n",
      "acid team\n",
      "afebrile glucose\n",
      "admission runs\n",
      "action oriented\n"
     ]
    }
   ],
   "source": [
    "### LR ANALYSIS \n",
    "\n",
    "best_unigram_model = models_unigram[0]\n",
    "best_bigram_model = models_bigram[0]\n",
    "best_unibigram_model = models_unigram[0]\n",
    "\n",
    "feature_names_unigram = np.array(cv_unigram.get_feature_names())\n",
    "feature_names_bigram = np.array(cv_bigram.get_feature_names())\n",
    "feature_names_unibigram = np.array(cv.get_feature_names())\n",
    "\n",
    "num_top = 30\n",
    "def find_top_features(model, feature_names):\n",
    "    coefs=model.coef_[0]\n",
    "    top = np.argpartition(coefs, -num_top)[-num_top:]\n",
    "    top_sorted=top[np.argsort(coefs[top])]\n",
    "    names = feature_names[top_sorted]\n",
    "\n",
    "    print \"Top Features\"\n",
    "    for i in range(num_top):\n",
    "        index = num_top - 1 - i\n",
    "        print names[index]\n",
    "\n",
    "print \"Top Features for Unigram\"\n",
    "unigram_features = np.array(cv_unigram.get_feature_names())\n",
    "find_top_features(best_unigram_model, unigram_features)\n",
    "\n",
    "print \"Top Features for Bigram\"\n",
    "bigram_features = np.array(cv_bigram.get_feature_names())\n",
    "find_top_features(best_bigram_model, bigram_features)\n",
    "\n",
    "print \"Top Features for Unigram and Bigram\"\n",
    "unibigram_features = np.array(cv.get_feature_names())\n",
    "find_top_features(best_unibigram_model, unibigram_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ### SVM Classifier \n",
    "# from sklearn import svm\n",
    "\n",
    "# # testing scores \n",
    "# test_svm_u = []\n",
    "# test_svm_b = []\n",
    "# test_svm_ub = []\n",
    "\n",
    "# # models \n",
    "# models_svm_u = []\n",
    "# models_svm_b = []\n",
    "# models_svm_ub = [] \n",
    "\n",
    "# # training scores \n",
    "# train_svm_u = []\n",
    "# train_svm_b = []\n",
    "# train_svm_ub = [] \n",
    "\n",
    "# for c in constants:\n",
    "#     # unigram\n",
    "#     svm_u = svm.SVC()\n",
    "#     svm_u.fit(cv_unigram_fit, train_labels)\n",
    "#     train_svm_u.append(svm_u.score(cv_unigram_fit, train_labels))\n",
    "#     test_svm_u.append(svm_u.score(cv_unigram_test, test_labels))\n",
    "#     models_svm_u.append(svm_u)\n",
    "    \n",
    "#     # bigram\n",
    "#     svm_b = svm.SVC()\n",
    "#     svm_b.fit(cv_bigram_fit, train_labels)\n",
    "#     train_svm_b.append(svm_b.score(cv_bigram_fit, train_labels))\n",
    "#     test_svm_b.append(svm_b.score(cv_bigram_test, test_labels))\n",
    "#     models_svm_b.append(svm_b)\n",
    "    \n",
    "#     # unibigram\n",
    "#     svm_ub = svm.SVC()\n",
    "#     svm_ub.fit(cv_fit, train_labels)\n",
    "#     train_svm_ub.append(svm_ub.score(cv_fit, train_labels))\n",
    "#     test_svm_ub.append(svm_ub.score(cv_test, test_labels))\n",
    "#     models_svm_ub.append(svm_ub)\n",
    "\n",
    "# print \"Testing Scores\"\n",
    "# print \"Scores for Unigram\", test_svm_u\n",
    "# print \"Scores for Bigram\", test_svm_b \n",
    "# print \"Scores for Unigram and Bigram\", test_svm_ub\n",
    "\n",
    "# print \"Training Scores\"\n",
    "# print \"Scores for Unigram\", train_svm_u\n",
    "# print \"Scores for Bigram\", train_svm_b\n",
    "# print \"Scores for Unigram and Bigram\", train_svm_ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### SVM ANALYSIS \n",
    "\n",
    "# best_unigram_svm = models_svm_u[]\n",
    "# best_bigram_svm = models_svm_b[]\n",
    "# best_unibigram_svm = models_svm_ub[]\n",
    "\n",
    "# print \"Top Features for SVM Unigram\"\n",
    "# find_top_features(best_unigram_svm, unigram_features)\n",
    "\n",
    "# print \"Top Features for SVM Bigram\"\n",
    "# find_top_features(best_bigram_svm, bigram_features)\n",
    "\n",
    "# print \"Top Features for SVM Unigram and Bigram\"\n",
    "# find_top_features(best_unibigram_svm, unibigram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Scores for Tree Classifiers\n",
      "Scores for DT []\n",
      "Scores for ET [0.69974086453599726, 0.74649791564952872, 0.75051639313478802, 0.69974086453599726, 0.74649791564952872, 0.75051639313478802, 0.69974086453599726, 0.74649791564952872, 0.75051639313478802, 0.69974086453599726, 0.74649791564952872, 0.75051639313478802, 0.69974086453599726, 0.74649791564952872, 0.75051639313478802]\n",
      "Scores for RF []\n",
      "Training Scores\n",
      "Scores for DT []\n",
      "Scores for ET [0.99975057679117041, 0.99975057679117041, 0.99927667269439424, 0.99975057679117041, 0.99975057679117041, 0.99927667269439424, 0.99975057679117041, 0.99975057679117041, 0.99927667269439424, 0.99975057679117041, 0.99975057679117041, 0.99927667269439424, 0.99975057679117041, 0.99975057679117041, 0.99927667269439424]\n",
      "Scores for RF []\n"
     ]
    }
   ],
   "source": [
    "### RANDOM FORESTS \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# testing scores \n",
    "test_t_u = []\n",
    "test_t_b = []\n",
    "test_t_ub = []\n",
    "\n",
    "# models \n",
    "models_t_u = []\n",
    "models_t_b = []\n",
    "models_t_ub = [] \n",
    "\n",
    "# training scores \n",
    "train_t_u = []\n",
    "train_t_b = []\n",
    "train_t_ub = [] \n",
    "\n",
    "for c in constants:\n",
    "    \n",
    "    # DT\n",
    "    dt = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "    dt.fit(cv_bigram_fit, train_labels)\n",
    "    train_t_b.append(dt.score(cv_bigram_fit, train_labels))\n",
    "    test_t_b.append(dt.score(cv_bigram_test, test_labels))\n",
    "    models_t_b.append(dt)\n",
    "    \n",
    "    # ET\n",
    "    et = ExtraTreesClassifier(max_depth=None, min_samples_split=2, random_state=0) \n",
    "    et.fit(cv_bigram_fit, train_labels)\n",
    "    train_t_b.append(et.score(cv_bigram_fit, train_labels))\n",
    "    test_t_b.append(et.score(cv_bigram_test, test_labels))\n",
    "    models_t_b.append(et)\n",
    "    \n",
    "    # RF\n",
    "    rf = RandomForestClassifier(max_depth=None, min_samples_split=2, random_state=0) \n",
    "    rf.fit(cv_bigram_fit, train_labels)\n",
    "    train_t_b.append(rf.score(cv_bigram_fit, train_labels))\n",
    "    test_t_b.append(rf.score(cv_bigram_test, test_labels))\n",
    "    models_t_b.append(rf)\n",
    "\n",
    "print \"Testing Scores for Tree Classifiers\"\n",
    "print \"Scores for DT\", test_t_u\n",
    "print \"Scores for ET\", test_t_b \n",
    "print \"Scores for RF\", test_t_ub\n",
    "\n",
    "print \"Training Scores\"\n",
    "print \"Scores for DT\", train_t_u\n",
    "print \"Scores for ET\", train_t_b\n",
    "print \"Scores for RF\", train_t_ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Random Forests ANALYSIS \n",
    "best_unigram_rf = models_t_b[0]\n",
    "best_bigram_rf = models_t_b[1]\n",
    "best_unibigram_rf = models_t_b[2]\n",
    "\n",
    "print \"Top Features for RF Unigram\"\n",
    "find_top_features(best_unigram_rf, unigram_features)\n",
    "\n",
    "print \"Top Features for RF Bigram\"\n",
    "find_top_features(best_bigram_rf, bigram_features)\n",
    "\n",
    "print \"Top Features for RF Unigram and Bigram\"\n",
    "find_top_features(best_unibigram_rf, unibigram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
